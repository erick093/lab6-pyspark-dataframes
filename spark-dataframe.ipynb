{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, explode, desc, split\n",
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField, ArrayType\n",
    "from time import time\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PySpark - DataFrames\n",
    "\n",
    "En este laboratorio utilizaremos Spark SQL. Spark SQL es un modulo de Spark que nos permite trabajar con una abstraccion de datos llamada DataFrame. Un DataFrame es una estructura de datos distribuida en filas, con nombre de columnas y tipos de datos. Los DataFrames son similares a las tablas en una base de datos relacional o las tablas en un archivo Excel, con la diferencia de que los DataFrames pueden ser distribuidos en multiples nodos de un cluster."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "ss = SparkSession.builder.getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0xffff64bbf8b0>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>pyspark-shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RDD vs DataFrame\n",
    "En nuestro notebook anterior, hemos trabajado con RDD (Conjunto de datos distribuido resistente), la abstracción básica en Spark.\n",
    "\n",
    "En esta sesión de laboratorio, estudiamos (o revisamos) la API de DataFrame, una colección de datos distribuidos e inmutables. Los DataFrames permiten a los desarrolladores imponer una estructura en una colección distribuida de datos, lo que permite una abstracción de mayor nivel; Además, DataFrames proporciona una API de lenguaje específico de dominio (DSL) para manipular datos estructurados y distribuidos. En última instancia, el objetivo es hacer que Spark sea accesible a un público más amplio, más allá de los investigadores y los ingenieros de datos especializados."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "sc = ss.sparkContext"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 40000\n",
      "Elapsed time: 1.207739\n"
     ]
    }
   ],
   "source": [
    "time_start = time()\n",
    "input_file = sc.textFile(\"datasets/tiny-shakespeare.txt\")\n",
    "num_lines = input_file.count()\n",
    "time_end = time()\n",
    "print(\"Number of lines: %i\" % (num_lines))\n",
    "print(\"Elapsed time: %f\" % (time_end - time_start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 40000\n",
      "Elapsed time: 0.235776\n"
     ]
    }
   ],
   "source": [
    "time_start = time()\n",
    "input_file = ss.read.text(\"datasets/tiny-shakespeare.txt\")\n",
    "num_lines = input_file.count()\n",
    "time_end = time()\n",
    "print(\"Number of lines: %i\" % (num_lines))\n",
    "print(\"Elapsed time: %f\" % (time_end - time_start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preguntas\n",
    "Usando el Web UI de Spark, responda las siguientes preguntas:\n",
    "\n",
    "1. ¿Cuántos trabajos se ejecutaron para contar el número de líneas en el archivo tiny-shakespeare.txt?\n",
    "2. ¿Por qué el tiempo de ejecución es diferente para RDD y DataFrame? Explique."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ejercicio 1: Contar palabras\n",
    "\n",
    "En este ejercicio, obtendremos la frecuencia de palabras en el archivo tiny-shakespeare.txt. Mostraremos las 10 palabras más comunes en el archivo."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words: [(7241, ''), (5437, 'the'), (4403, 'I'), (3923, 'to'), (3678, 'and'), (3275, 'of'), (2677, 'my'), (2610, 'a'), (2130, 'you'), (2073, 'in')]\n",
      "Elapsed time: 2.729319\n"
     ]
    }
   ],
   "source": [
    "time_start = time()\n",
    "words = sc.textFile(\"datasets/tiny-shakespeare.txt\").repartition(8)\\\n",
    "            .flatMap(lambda line: line.split(\" \"))\\\n",
    "            .map(lambda word: (word, 1))\\\n",
    "            .reduceByKey(lambda a, b: a + b)\\\n",
    "            .map(lambda x: (x[1], x[0]))\\\n",
    "            .sortByKey(False)\n",
    "\n",
    "top_words = words.take(10)\n",
    "time_end = time()\n",
    "\n",
    "print(\"Top words: %s\" % (top_words))\n",
    "print(\"Elapsed time: %f\" % (time_end - time_start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words: [Row(word='', count=7241), Row(word='the', count=5437), Row(word='I', count=4403), Row(word='to', count=3923), Row(word='and', count=3678), Row(word='of', count=3275), Row(word='my', count=2677), Row(word='a', count=2610), Row(word='you', count=2130), Row(word='in', count=2073)]\n",
      "Elapsed time: 1.268705\n"
     ]
    }
   ],
   "source": [
    "time_start = time()\n",
    "words = ss.read.text(\"datasets/tiny-shakespeare.txt\").repartition(8)\\\n",
    "            .selectExpr(\"split(value, ' ') as words\")\\\n",
    "            .select(explode(col(\"words\")).alias(\"word\"))\\\n",
    "            .groupBy(\"word\")\\\n",
    "            .count()\\\n",
    "            .orderBy(col(\"count\").desc())\n",
    "\n",
    "top_words = words.take(10)\n",
    "time_end = time()\n",
    "\n",
    "print(\"Top words: %s\" % (top_words))\n",
    "print(\"Elapsed time: %f\" % (time_end - time_start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words: [Row(word='', count=7241), Row(word='the', count=5437), Row(word='I', count=4403), Row(word='to', count=3923), Row(word='and', count=3678), Row(word='of', count=3275), Row(word='my', count=2677), Row(word='a', count=2610), Row(word='you', count=2130), Row(word='in', count=2073)]\n",
      "Elapsed time: 1.228585\n"
     ]
    }
   ],
   "source": [
    "time_start = time()\n",
    "words = ss.read.text(\"datasets/tiny-shakespeare.txt\").repartition(8)\\\n",
    "            .selectExpr(\"split(value, ' ') as words\")\\\n",
    "            .select(explode(col(\"words\")).alias(\"word\"))\\\n",
    "            .groupBy(\"word\")\\\n",
    "            .count()\\\n",
    "            .orderBy(col(\"count\").desc())\n",
    "top_words = words.take(10)\n",
    "time_end = time()\n",
    "\n",
    "print(\"Top words: %s\" % (top_words))\n",
    "print(\"Elapsed time: %f\" % (time_end - time_start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preguntas\n",
    "Usando el Web UI de Spark, responda las siguientes preguntas:\n",
    "\n",
    "1. ¿Por qué el tiempo de ejecución es diferente para RDD y DataFrame? Explique.\n",
    "2. ¿Cual es el API de Spark que es mas eficiente (RDD o DataFrame)? Explique."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ejercicio 2: Flights\n",
    "\n",
    "Un Dataframe es una colección de datos distribuidos e inmutables, organizados en columnas con nombres y tipos de datos. Los DataFrames pueden ser creados a partir de una variedad de fuentes, como archivos, tablas de Hive, tablas de HBase, RDDs, etc. En este ejercicio, trabajaremos con un dataset de informacion de vuelos. Usaremos el mismo dataset del laboratorio anterior.\n",
    "\n",
    "Para construir el dataframe hay varias formas:\n",
    "\n",
    "* Usando el metodo `read.csv` de SparkSession\n",
    "* Usando el metodo `read.format(\"csv\").load` de SparkSession\n",
    "* Usando el metodo `read.load` de SparkSession\n",
    "\n",
    "En este ejercicio, se pide leer el archivo `2008.csv` usando los tres metodos y comparar los tiempos de ejecucion. Usar la inferencia de tipos de datos para leer el archivo.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" YOUR CODE HERE \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preguntas\n",
    "\n",
    "1. ¿Cual es el metodo mas eficiente para leer el archivo? Explique.\n",
    "2. ¿Por que la diferencia de tiempo de ejecucion? Explique."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nuestra siguiente tarea es leer los archivos sin usar la inferencia de datos. Para esto deberemos crear el esquema manualmente.\n",
    "\n",
    "El esquema es una estructura de datos que define los nombres y tipos de datos de las columnas de un dataframe. Para crear el esquema, usaremos la clase `StructType` y la clase `StructField`. La clase `StructType` representa un esquema de datos y la clase `StructField` representa una columna del esquema. Para crear un esquema, primero creamos una lista de objetos `StructField` y luego pasamos la lista al constructor de `StructType`.\n",
    "\n",
    "El constructor de `StructField` toma tres argumentos: el nombre de la columna, el tipo de datos de la columna y un booleano que indica si la columna puede tener valores nulos. Los tipos de datos soportados por Spark son: `StringType`, `IntegerType`, `LongType`, `FloatType`, `DoubleType`, `BooleanType`, `DateType`, `TimestampType`, `BinaryType`, `DecimalType`, `ArrayType`, `MapType`, `StructType` y `NullType`. Para mas informacion, ver la documentacion de [Spark SQL Types](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" YOUR CODE HERE \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preguntas\n",
    "1. ¿Por que la diferencia de tiempo de ejecucion entre los metodos que utilizan la inferencia de datos y los que no? Explique.\n",
    "2. ¿Cual es el metodo mas eficiente para leer el archivo en el cual especificamos manualmente esquema? Explique."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Queries - Simples\n",
    "\n",
    "1. ¿Cuantos aeropuertos de destino diferentes hay? ¿Cuantos aeropuertos de origen diferentes hay? Comentar si hay algun aeropuerto que sea de origen y destino al mismo tiempo.\n",
    "2. ¿Cuantas aerolineas diferentes hay?\n",
    "3. ¿Cuantos vuelos existen que fueron agendados para salir despues de las 19:00?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" YOUR CODE HERE \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Queries - Estadisticas sobre el volumen de vuelos.\n",
    "\n",
    "1. ¿Cuantos vuelos hay en cada mes del año?\n",
    "2. ¿Hay alguna relacion entre el dia de la semana y el volumen de vuelos? Comenta y justifica tu respuesta.\n",
    "3. ¿Cuantos vuelos salen en cada hora del dia? Considera la hora de salida agendada. Analiza los resultados, hay algun resultado que llame tu atencion? Comenta.\n",
    "4. ¿Cuales son los 10 aeropuertos de origen con mayor volumen de vuelos? ( Esto depende del numero de vuelos que salen Y llegan a ese aeropuerto)\n",
    "5. ¿Cuales son las 10 aerolineas con mayor volumen de vuelos?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" YOUR CODE HERE \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Queries - Estadisticas sobre los vuelos atrasados\n",
    "\n",
    "1. ¿Cual es el porcentaje de vuelos retrasados (sobre el total de vuelos) hay por cada hora del dia? Considera la hora de salida agendada.\n",
    "2. ¿Que horas del dia tienen son las mas propensas a tener los retrasos mas largos? Comenta.\n",
    "3. ¿Cual es el porcentaje de vuelos retrasados que salen de uno de los 10 aeropuertos de origen con mayor volumen de vuelos (vuelos de entrada y salida)? Comenta."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" YOUR CODE HERE \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}